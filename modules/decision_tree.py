import streamlit as st 
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, roc_auc_score
from sklearn.preprocessing import label_binarize

def decision_tree_module(df):
    st.subheader("Configuraci√≥n del √Årbol de Decisi√≥n")
    
    # --- Preparaci√≥n de datos ---
    st.write("**Preparaci√≥n de datos:**")
    
    col1, col2 = st.columns(2)
    
    with col1:
        # Selecci√≥n de variable objetivo
        target_col = st.selectbox(
            "Selecciona la variable objetivo:",
            df.columns,
            key="target_select_dt"
        )
    
    with col2:
        # Selecci√≥n de variables predictoras
        available_features = [col for col in df.columns if col != target_col]
        selected_features = st.multiselect(
            "Selecciona las variables predictoras:",
            available_features,
            default=available_features,
            key="features_select_dt"
        )
    
    if not selected_features:
        st.warning("‚ö†Ô∏è Selecciona al menos una variable predictora")
        return
    
    # Construir X, y
    X = df[selected_features]
    y = df[target_col]
    
    # --- Limpieza y procesamiento de datos ---
    st.write("**Limpieza y procesamiento de datos:**")
    
    # Limpieza de valores nulos
    missing_rows = X.isnull().any(axis=1) | y.isnull()
    if missing_rows.any():
        st.warning(f"‚ö†Ô∏è Se eliminaron {missing_rows.sum()} filas con valores nulos")
        X = X[~missing_rows]
        y = y[~missing_rows]
    
    # Manejo de variables categ√≥ricas
    categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()
    if categorical_cols:
        st.info(f"‚ÑπÔ∏è Variables categ√≥ricas detectadas: {', '.join(categorical_cols)}")
        try:
            X = pd.get_dummies(X, drop_first=True)
            st.success(f"‚úÖ Variables convertidas a one-hot encoding. Nuevas dimensiones: {X.shape}")
        except Exception as e:
            st.error(f"‚ùå Error al convertir variables categ√≥ricas: {str(e)}")
            return
    else:
        st.info("‚ÑπÔ∏è No se detectaron variables categ√≥ricas en los predictores")
    
    # Procesamiento de variable objetivo
    y = y.astype(str).str.strip()
    y = y.replace("?", np.nan)
    mask = ~y.isna()
    X = X[mask]
    y = y[mask]

    # Eliminaci√≥n de clases raras
    class_counts = y.value_counts()
    rare_classes = class_counts[class_counts < 2].index
    if len(rare_classes) > 0:
        st.warning(f"‚ö†Ô∏è Se eliminaron {len(rare_classes)} clases con menos de 2 muestras")
        mask = ~y.isin(rare_classes)
        X = X[mask]
        y = y[mask]
    
    # Validaci√≥n final
    if len(y.unique()) < 2:
        st.error("‚ùåNo hay suficientes clases para entrenar el modelo. Se necesitan al menos 2 clases diferentes.")
        return
    
    # --- Configuraci√≥n de divisi√≥n de datos ---
    st.write("**Configuraci√≥n de divisi√≥n de datos:**")
    
    col1, col2 = st.columns(2)
    
    with col1:
        test_size = st.slider("Tama√±o del conjunto de prueba:", 0.1, 0.4, 0.2, 0.05)
    
    with col2:
        random_state = st.number_input("Random state:", 0, 100, 42)
    
    # Validar estratificaci√≥n
    can_stratify = len(y.unique()) >= 2 and y.value_counts().min() >= 2
    
    # Divisi√≥n del dataset
    try:
        if can_stratify:
            X_train, X_test, y_train, y_test = train_test_split(
                X, y,
                test_size=test_size,
                random_state=random_state,
                stratify=y,
                shuffle=True
            )
            st.success("‚úÖ Divisi√≥n estratificada realizada correctamente")
        else:
            X_train, X_test, y_train, y_test = train_test_split(
                X, y,
                test_size=test_size,
                random_state=random_state,
                shuffle=True
            )
            st.info("‚ÑπÔ∏è Divisi√≥n no estratificada realizada")
    except ValueError as e:
        st.warning(f"‚ö†Ô∏è Divisi√≥n estratificada fall√≥: {e}. Reintentando sin estratificaci√≥n...")
        X_train, X_test, y_train, y_test = train_test_split(
            X, y,
            test_size=test_size,
            random_state=random_state,
            shuffle=True
        )
    
    # --- Configuraci√≥n de hiperpar√°metros ---
    st.subheader("Configuraci√≥n de Hiperpar√°metros")
    
    col1, col2 = st.columns(2)
    
    with col1:
        criterion = st.selectbox(
            "Criterio de divisi√≥n:",
            ["gini", "entropy"],
            help="GINI: medida de impureza | Entropy: ganancia de informaci√≥n"
        )
        max_depth = st.slider("Profundidad m√°xima:", 1, 20, 5)
    
    with col2:
        min_samples_split = st.slider("M√≠nimo samples para split:", 2, 20, 2)
        min_samples_leaf = st.slider("M√≠nimo samples por hoja:", 1, 10, 1)
    
    # --- Entrenamiento del modelo ---
    if st.button("Entrenar √Årbol de Decisi√≥n", type="primary"):
        try:
            if len(np.unique(y_train)) < 2:
                st.error("‚ùå El conjunto de entrenamiento debe tener al menos 2 clases diferentes")
                return
            
            with st.spinner("Entrenando modelo..."):
                model = DecisionTreeClassifier(
                    criterion=criterion,
                    max_depth=max_depth,
                    min_samples_split=min_samples_split,
                    min_samples_leaf=min_samples_leaf,
                    random_state=random_state
                )
                model.fit(X_train, y_train)
                
                y_pred = model.predict(X_test)
                try:
                    y_prob = model.predict_proba(X_test)
                except Exception:
                    y_prob = None
                
                st.success("‚úÖ Modelo entrenado exitosamente")
                display_results(y_test, y_pred, y_prob, model.classes_)
        
        except Exception as e:
            st.error(f"‚ùå Error al entrenar el modelo: {str(e)}")
            st.write("Sugerencia: Verifica la variable objetivo, ajusta los hiperpar√°metros o revisa las variables predictoras.")

def display_results(y_test, y_pred, y_prob, classes):
    st.subheader("Resultados de la Evaluaci√≥n")
    
    # Determinar el tipo de problema
    n_classes = len(classes)
    is_binary_classification = n_classes == 2
    is_multiclass_classification = n_classes > 2
    
    # Crear pesta√±as seg√∫n el tipo de problema
    if is_binary_classification and y_prob is not None:
        tab1, tab2, tab3 = st.tabs(["Matriz de Confusi√≥n", "Reporte de Clasificaci√≥n", "Curva ROC y AUC"])
    else:
        tab1, tab2 = st.tabs(["Matriz de Confusi√≥n", "Reporte de Clasificaci√≥n"])
    
    # Matriz de Confusi√≥n
    with tab1:
        st.write("**Matriz de Confusi√≥n:**")
        cm = confusion_matrix(y_test, y_pred)

        num_classes = len(classes)

        # üîß AJUSTES DIN√ÅMICOS MEJORADOS para matriz de confusi√≥n
        if num_classes == 2:
            # Tama√±o especial para 2 clases (como sex)
            fig_width, fig_height = 6, 5
            font_size = 16
            tick_font_size = 14
            annotation_size = 18
        elif num_classes <= 5:
            fig_width, fig_height = 8, 7
            font_size = 14
            tick_font_size = 12
            annotation_size = 14
        elif num_classes <= 10:
            fig_width = min(1 + num_classes * 0.5, 12)
            fig_height = min(1 + num_classes * 0.5, 12)
            font_size = 10
            tick_font_size = 10
            annotation_size = 10
        else:
            fig_width = min(1 + num_classes * 0.5, 20)
            fig_height = min(1 + num_classes * 0.5, 20)
            font_size = 8
            tick_font_size = 8
            annotation_size = 8

        fig, ax = plt.subplots(figsize=(fig_width, fig_height))

        # Crear heatmap con mejoras visuales
        heatmap = sns.heatmap(
            cm,
            annot=True,
            fmt='d',
            cmap='Blues',
            xticklabels=classes,
            yticklabels=classes,
            annot_kws={'size': annotation_size, 'weight': 'bold'},
            cbar_kws={'shrink': 0.7},
            square=True  # Hace la matriz cuadrada para mejor aspecto
        )

        # Mejorar etiquetas y t√≠tulo
        ax.set_xlabel('Predicciones', fontsize=tick_font_size + 2, weight='bold')
        ax.set_ylabel('Valores Reales', fontsize=tick_font_size + 2, weight='bold')
        ax.set_title('Matriz de Confusi√≥n', fontsize=font_size + 4, weight='bold', pad=20)

        # Rotar etiquetas solo si hay muchas clases
        if num_classes > 5:
            rotation = 45
            ha = 'right'
        else:
            rotation = 0
            ha = 'center'

        ax.set_xticklabels(ax.get_xticklabels(), rotation=rotation, ha=ha, fontsize=tick_font_size)
        ax.set_yticklabels(ax.get_yticklabels(), rotation=0, fontsize=tick_font_size)

        # A√±adir l√≠neas de separaci√≥n m√°s visibles para pocas clases
        if num_classes <= 5:
            for i in range(num_classes + 1):
                ax.axhline(i, color='white', linewidth=2)
                ax.axvline(i, color='white', linewidth=2)

        plt.tight_layout()
        st.pyplot(fig)

        # Mostrar advertencia si hay demasiadas clases
        if num_classes > 15:
            st.warning("‚ö†Ô∏è Hay muchas clases en la variable objetivo. Considera agrupar clases similares para una mejor visualizaci√≥n.")

        # Informaci√≥n adicional para matrices peque√±as
        if num_classes == 2:
            st.info("""
            **üìä Interpretaci√≥n para 2 clases:**
            - **Verdaderos Negativos (TN)**: Casos negativos correctamente clasificados
            - **Falsos Positivos (FP)**: Casos negativos incorrectamente clasificados como positivos
            - **Falsos Negativos (FN)**: Casos positivos incorrectamente clasificados como negativos  
            - **Verdaderos Positivos (TP)**: Casos positivos correctamente clasificados
            """)

    # Reporte de Clasificaci√≥n Simplificado
    with tab2:
        st.subheader("Reporte de Clasificaci√≥n")
        
        if y_test is not None and y_pred is not None:
            # Calcular el reporte
            report = classification_report(y_test, y_pred, output_dict=True)
            
            accuracy = report.get('accuracy', 0)
            macro_avg = report.get('macro avg', {})
            weighted_avg = report.get('weighted avg', {})
            
            # Obtener m√©tricas por clase
            class_metrics = {}
            for key in report.keys():
                if key not in ['accuracy', 'macro avg', 'weighted avg'] and isinstance(report[key], dict):
                    class_metrics[key] = report[key]
            
            # M√©tricas Globales
            st.write("**M√©tricas Globales del Modelo**")
            
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric(
                    label="Exactitud (Accuracy)",
                    value=f"{accuracy:.3f}",
                    help="Porcentaje total de predicciones correctas"
                )
            
            with col2:
                precision_avg = weighted_avg.get('precision', macro_avg.get('precision', 0))
                st.metric(
                    label="Precisi√≥n Promedio",
                    value=f"{precision_avg:.3f}",
                    help="Capacidad del modelo para no predecir falsos positivos"
                )
            
            with col3:
                recall_avg = weighted_avg.get('recall', macro_avg.get('recall', 0))
                st.metric(
                    label="Recall Promedio", 
                    value=f"{recall_avg:.3f}",
                    help="Capacidad del modelo para encontrar todos los positivos"
                )
            
            with col4:
                f1_avg = weighted_avg.get('f1-score', macro_avg.get('f1-score', 0))
                st.metric(
                    label="F1-Score Promedio",
                    value=f"{f1_avg:.3f}",
                    help="Balance entre Precisi√≥n y Recall"
                )

            # Mostrar m√©tricas por clase si hay m√∫ltiples clases
            if class_metrics and len(class_metrics) > 1:
                class_metrics_df = pd.DataFrame(class_metrics).transpose()
                
                # Gr√°fico de rendimiento por clase
                st.write("**Rendimiento por Clase**")
                
                fig, ax = plt.subplots(figsize=(10, 5))
                
                classes = class_metrics_df.index
                x = np.arange(len(classes))
                width = 0.25
                
                # Crear barras para cada m√©trica
                bars1 = ax.bar(x - width, class_metrics_df['precision'], width, label='Precisi√≥n', alpha=0.8, color='#FF6B6B')
                bars2 = ax.bar(x, class_metrics_df['recall'], width, label='Recall', alpha=0.8, color='#4ECDC4')
                bars3 = ax.bar(x + width, class_metrics_df['f1-score'], width, label='F1-Score', alpha=0.8, color='#45B7D1')
                
                # Personalizar gr√°fico
                ax.set_xlabel('Clases')
                ax.set_ylabel('Puntuaci√≥n')
                ax.set_title('M√©tricas por Clase')
                ax.set_xticks(x)
                ax.set_xticklabels(classes, rotation=45, ha='right')
                ax.legend()
                ax.set_ylim(0, 1.05)
                ax.grid(True, alpha=0.3, axis='y')
                
                plt.tight_layout()
                st.pyplot(fig)
                
                # Tabla de m√©tricas
                st.write("**Tabla de M√©tricas por Clase**")
                
                class_metrics_display = class_metrics_df.copy()
                class_metrics_display.index.name = 'Clase'
                class_metrics_display = class_metrics_display.reset_index()
                
                st.dataframe(
                    class_metrics_display.style.format({
                        'precision': '{:.3f}',
                        'recall': '{:.3f}', 
                        'f1-score': '{:.3f}',
                        'support': '{:.0f}'
                    }),
                    use_container_width=True,
                    height=min(300, 100 + len(class_metrics_df) * 35)
                )
            
            elif class_metrics and len(class_metrics) == 1:
                # Caso binario
                st.info("Clasificaci√≥n binaria")
                class_name = list(class_metrics.keys())[0]
                metrics = class_metrics[class_name]
                
                st.write(f"**M√©tricas para la clase {class_name}:**")
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("Precisi√≥n", f"{metrics['precision']:.3f}")
                with col2:
                    st.metric("Recall", f"{metrics['recall']:.3f}")
                with col3:
                    st.metric("F1-Score", f"{metrics['f1-score']:.3f}")

        else:
            st.warning("No hay datos de prueba o predicciones disponibles")
            
    # Curva ROC y AUC solo para clasificaci√≥n binaria
    if is_binary_classification and y_prob is not None:
        with tab3:
            with st.expander("Curva ROC (Receiver Operating Characteristic)"):
                st.markdown("""
            La **Curva ROC** es una representaci√≥n gr√°fica que muestra la capacidad de un clasificador 
            para diferenciar entre clases. Se basa en dos m√©tricas:
            
            - **Tasa de falsos positivos (False Positive Rate - FPR):** Proporci√≥n de negativos incorrectamente clasificados como positivos.
            - **Tasa de verdaderos positivos (True Positive Rate - TPR):** Proporci√≥n de positivos correctamente identificados.
            
            La curva muestra la relaci√≥n entre TPR y FPR para diferentes umbrales de decisi√≥n.
            
            **Nota:** La curva ROC solo est√° disponible para problemas de clasificaci√≥n binaria.
            """)
            
            try:
                # --- CORRECCI√ìN PRINCIPAL: Convertir y_test a num√©rico ---
                # Crear mapeo de clases a n√∫meros
                class_mapping = {class_name: i for i, class_name in enumerate(classes)}
                y_test_numeric = np.array([class_mapping[label] for label in y_test])
                
                # --- GR√ÅFICO: CURVA ROC ---
                st.subheader("Curva ROC")

                # Ajustar tama√±o de figura
                fig_width = 10
                fig_height = 8
                fig_roc, ax_roc = plt.subplots(figsize=(fig_width, fig_height))

                # Clasificaci√≥n binaria - CORREGIDO
                # Para clasificaci√≥n binaria, usar la segunda clase como positiva
                pos_label = 1  # Segunda clase en el mapeo
                
                fpr, tpr, _ = roc_curve(y_test_numeric, y_prob[:, pos_label], pos_label=pos_label)
                roc_auc = auc(fpr, tpr)
                
                ax_roc.plot(fpr, tpr, color='darkorange', lw=3, label=f'Curva ROC (AUC = {roc_auc:.3f})')
                ax_roc.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='L√≠nea base (AUC = 0.5)')
                ax_roc.set_xlim([0.0, 1.0])
                ax_roc.set_ylim([0.0, 1.05])
                ax_roc.set_xlabel('Tasa de Falsos Positivos (FPR)', fontsize=11)
                ax_roc.set_ylabel('Tasa de Verdaderos Positivos (TPR)', fontsize=11)
                ax_roc.set_title('Curva ROC - Clasificaci√≥n Binaria', fontsize=13, fontweight='bold')
                
                # Mostrar qu√© clase se considera positiva
                positive_class = classes[pos_label]
                negative_class = classes[0]
                ax_roc.text(0.02, 0.98, f'Clase positiva: {positive_class}\nClase negativa: {negative_class}', 
                           transform=ax_roc.transAxes, fontsize=10, 
                           bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
                
                # Leyenda fuera del gr√°fico
                ax_roc.legend(loc='center left', bbox_to_anchor=(1.05, 0.5), fontsize=10)
                ax_roc.grid(True, alpha=0.3)
                
                # Ajustar tama√±o de ticks
                ax_roc.tick_params(axis='both', which='major', labelsize=10)

                # Ajustar el layout para hacer espacio para la leyenda
                plt.tight_layout(rect=[0, 0, 0.85, 1])
                st.pyplot(fig_roc)
                
                with st.expander("√Årea bajo la curva (AUC)"):
                    st.markdown("""
                El **AUC** cuantifica la calidad de la curva ROC en un solo valor:
                
                - **0.9 - 1.0:** Excelente poder discriminativo
                - **0.8 - 0.9:** Muy bueno
                - **0.7 - 0.8:** Aceptable
                - **0.6 - 0.7:** Pobre
                - **0.5 - 0.6:** No mejor que aleatorio
                - **< 0.5:** Peor que aleatorio
                """)
                        
                # --- MOSTRAR M√âTRICAS NUM√âRICAS ---
                st.subheader("M√©tricas de Evaluaci√≥n AUC")
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("AUC Score", f"{roc_auc:.4f}")
                with col2:
                    st.metric("Interpretaci√≥n", interpretar_auc(roc_auc))
                with col3:
                    quality = "‚úÖ Excelente" if roc_auc >= 0.9 else "üëç Buena" if roc_auc >= 0.8 else "‚ö†Ô∏è Aceptable" if roc_auc >= 0.7 else "‚ùå Pobre"
                    st.metric("Calidad", quality)
                
                # Gr√°fico de m√©tricas AUC
                fig_auc, ax_auc = plt.subplots(figsize=(8, 6))
                
                metrics_data = [roc_auc]
                metric_labels = ['AUC']
                colors = ['lightgreen' if roc_auc >= 0.7 else 'lightcoral']
                
                bars = ax_auc.bar(metric_labels, metrics_data, color=colors, edgecolor='black', alpha=0.8)
                ax_auc.axhline(y=0.5, color='red', linestyle='--', alpha=0.7, label='Aleatorio')
                ax_auc.axhline(y=0.7, color='orange', linestyle='--', alpha=0.7, label='Aceptable')
                ax_auc.axhline(y=0.9, color='green', linestyle='--', alpha=0.7, label='Excelente')
                ax_auc.set_ylim(0, 1.1)
                ax_auc.set_ylabel('Valor AUC')
                ax_auc.set_title('M√©trica AUC - Clasificaci√≥n Binaria')
                ax_auc.legend()
                
                # A√±adir valores en las barras
                for bar, v in zip(bars, metrics_data):
                    height = bar.get_height()
                    ax_auc.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                            f'{v:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=12)
                
                plt.tight_layout()
                st.pyplot(fig_auc)
                        
            except Exception as e:
                st.error(f"‚ùå Error al calcular las curvas ROC: {str(e)}")
                st.info("‚ÑπÔ∏è Esto puede ocurrir cuando hay problemas con las probabilidades predichas o las clases objetivo")
    
    # Mensaje informativo para problemas multiclase
    elif is_multiclass_classification:
        st.info("""
        **‚ÑπÔ∏è Informaci√≥n sobre M√©tricas para Clasificaci√≥n Multiclase:**
        
        Para problemas de clasificaci√≥n con m√°s de 2 clases:
        - La **Curva ROC y AUC** no se muestran ya que son m√©tricas dise√±adas principalmente para clasificaci√≥n binaria
        - En su lugar, se recomienda usar las m√©tricas mostradas en el Reporte de Clasificaci√≥n (Precisi√≥n, Recall, F1-Score)
        - La Matriz de Confusi√≥n proporciona una visi√≥n detallada del rendimiento por clase
        """)

def interpretar_auc(auc_score):
    """Funci√≥n auxiliar para interpretar scores AUC"""
    if auc_score >= 0.9:
        return "Excelente discriminaci√≥n"
    elif auc_score >= 0.8:
        return "Muy buena discriminaci√≥n"
    elif auc_score >= 0.7:
        return "Discriminaci√≥n aceptable"
    elif auc_score >= 0.6:
        return "Discriminaci√≥n pobre"
    elif auc_score >= 0.5:
        return "No mejor que aleatorio"
    else:
        return "Peor que aleatorio"